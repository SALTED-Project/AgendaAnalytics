version: "3.1"
services:
  mongodb:
    image: kybeidosci/vdpp-mongodb:demonstrator_v0.0.1
    restart: always
    extra_hosts:
      - "mongodb:127.0.0.1"
    networks:
      - backend
      - crawler
    volumes:
      - "mongo-data:/data/db"
    ports:
      - 27017:27017
    command: --auth --bind_ip_all --replSet "rs0" --setParameter "transactionLifetimeLimitSeconds=1800"

  middleware:
    image: kybeidosci/vdpp-middleware:demonstrator_v0.0.1
    depends_on:
      - mongodb
      - kafka
      - schema-registry
      - seaweedfs-master
      - seaweedfs-volume
    restart: unless-stopped
    networks:
      - backend
      - crawler
    ports:
      - "8083:8080"
    environment:
      DB_URI: mongodb://vdppmongouser:password@mongodb:27017/middleware?authSource=admin&replicaSet=rs0
      SEAWEED_MASTER_URL: http://seaweedfs-master:9333
      SEAWEED_VOLUME_SERVER_URL: http://seaweedfs-volume:8101
      DOCKER_NETWORK_INTERNAL_URL_OF_THIS: http://middleware:8080
      DOCKER_NETWORK_EXTERNAL_URL_OF_THIS: ${MIDDLEWARE_EXTERNAL_URL:-http://localhost:8083}
      KAFKA_BROKER_URLS: kafka:29092
      KAFKA_SCHEMA_REGISTRY_URL: http://schema-registry:8081
      PRE_START_PATH: /prestart.sh
      PORT: 8080
    deploy:  
      resources:
        limits:
          cpus: '0.25'
          memory: 500M

  seaweedfs-master:
    image: chrislusf/seaweedfs
    restart: always
    networks:
      - backend
    ports:
      - "9333:9333"
      - "19333:19333"
    volumes:
      - "seaweed-master-data:/data"
    command: 'master -ip=seaweedfs-master -port=9333 -mdir="/data"'

  seaweedfs-volume:
    image: chrislusf/seaweedfs
    restart: always
    depends_on:
      - seaweedfs-master
    networks:
      - backend
    ports:
      - "8101:8080"
      - "18080:18080"
    volumes:
      - "seaweed-volume-data:/data"
    environment:
      SEAWEED_VOLUME_PUBLIC_URL: localhost:8101
    command: 'volume -mserver="seaweedfs-master:9333" -ip=seaweedfs-volume -port=8080 -publicUrl=localhost:8101 -dir="/data"'

  crawler-interface:
    image: kybeidosci/vdpp-crawler-interface:demonstrator_v0.0.1
    restart: unless-stopped
    privileged: true
    networks:
      - backend
    depends_on:
      - mongodb
      - kafka
      - schema-registry
    environment:
      DYNACONF_MONGODB_HOST: mongodb
      DYNACONF_KAFKA_BROKER_URLS: kafka:29092
      DYNACONF_KAFKA_SCHEMA_REGISTRY_URL: http://schema-registry:8081
      DYNACONF_UPLOAD_URL: ${MIDDLEWARE_EXTERNAL_URL:-http://middleware:8080/files}
    command: [ "./install_lib_and_start.sh" ]

  crawler:
    image: 'kybeidosci/vdpp-crawler:demonstrator_v0.0.1'
    networks:
      - crawler
    depends_on:
      - storm-supervisor

  kafka-zookeeper:
    image: confluentinc/cp-zookeeper:5.5.0
    restart: always
    environment:
      ZOOKEEPER_CLIENT_PORT: 2181
      ZOOKEEPER_TICK_TIME: 5000
      ZOOKEEPER_AUTOPURGE_PURGE_INTERVAL: 24
      ZOOKEEPER_AUTOPURGE_SNAP_RETAIN_COUNT: 5
    volumes:
      - "kafka-zookeeper-data:/var/lib/zookeeper/data"
      - "kafka-zookeeper-logs:/var/lib/zookeeper/log"
    networks:
      - backend

  kafka:
    image: confluentinc/cp-server:5.5.0
    restart: always
    depends_on:
      - kafka-zookeeper
    ports:
      - 9092:9092
    environment:
      KAFKA_BROKER_ID: 1
      KAFKA_ZOOKEEPER_CONNECT: kafka-zookeeper:2181
      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: PLAINTEXT:PLAINTEXT,PLAINTEXT_HOST:PLAINTEXT
      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://kafka:29092,PLAINTEXT_HOST://localhost:9092
      KAFKA_METRIC_REPORTERS: io.confluent.metrics.reporter.ConfluentMetricsReporter
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1
      KAFKA_GROUP_INITIAL_REBALANCE_DELAY_MS: 0
      KAFKA_CONFLUENT_LICENSE_TOPIC_REPLICATION_FACTOR: 1
      KAFKA_TRANSACTION_STATE_LOG_MIN_ISR: 1
      KAFKA_TRANSACTION_STATE_LOG_REPLICATION_FACTOR: 1
      CONFLUENT_METRICS_REPORTER_BOOTSTRAP_SERVERS: kafka:29092
      CONFLUENT_METRICS_REPORTER_ZOOKEEPER_CONNECT: kafka-zookeeper:2181
      CONFLUENT_METRICS_REPORTER_TOPIC_REPLICAS: 1
      CONFLUENT_METRICS_ENABLE: 'true'
      CONFLUENT_SUPPORT_CUSTOMER_ID: 'anonymous'
    volumes:
      - "kafka-data:/var/lib/kafka/data"
    networks:
      - backend

  kafka-control-center:
    image: confluentinc/cp-enterprise-control-center:5.5.0
    restart: always
    depends_on:
      - kafka-zookeeper
      - kafka
      - schema-registry
    ports:
      - 9021:9021
    environment:
      CONTROL_CENTER_BOOTSTRAP_SERVERS: kafka:29092
      CONTROL_CENTER_ZOOKEEPER_CONNECT: kafka-zookeeper:2181
      CONTROL_CENTER_SCHEMA_REGISTRY_URL: http://schema-registry:8081
      CONTROL_CENTER_REPLICATION_FACTOR: 1
      CONTROL_CENTER_INTERNAL_TOPICS_PARTITIONS: 1
      CONTROL_CENTER_MONITORING_INTERCEPTOR_TOPIC_PARTITIONS: 1
      CONFLUENT_METRICS_TOPIC_REPLICATION: 1
      PORT: 9021
    networks:
      - backend

  schema-registry:
    image: confluentinc/cp-schema-registry:5.5.0
    restart: always
    depends_on:
      - kafka
      - kafka-zookeeper
    environment:
      SCHEMA_REGISTRY_HOST_NAME: schema-registry
      SCHEMA_REGISTRY_KAFKASTORE_CONNECTION_URL: kafka-zookeeper:2181
      SCHEMA_REGISTRY_LISTENERS: http://0.0.0.0:8081
    ports:
      - 8081:8081
    networks:
      - backend

  storm-zookeeper:
    image: confluentinc/cp-zookeeper:5.5.0
    restart: always
    environment:
      ZOOKEEPER_CLIENT_PORT: 2181
      ZOOKEEPER_TICK_TIME: 5000
      ZOOKEEPER_AUTOPURGE_PURGE_INTERVAL: 24
      ZOOKEEPER_AUTOPURGE_SNAP_RETAIN_COUNT: 5
    volumes:
      - "storm-zookeeper-data:/var/lib/zookeeper/data"
      - "storm-zookeeper-logs:/var/lib/zookeeper/log"
    networks:
      - crawler

  storm-nimbus:
    image: library/storm:1.2.3
    restart: always
    networks:
      - crawler
    ports:
      - 6627:6627
    depends_on:
      - "storm-zookeeper"
    volumes:
      - "storm-nimbus-data:/data"
    command: storm nimbus -c storm.zookeeper.servers='["storm-zookeeper"]'

  storm-supervisor:
    image: library/storm:1.2.3
    restart: always
    networks:
      - crawler
    depends_on:
      - "storm-nimbus"
      - "storm-zookeeper"
    volumes:
      - "storm-supervisor-data:/data"
    command: storm supervisor -c storm.zookeeper.servers='["storm-zookeeper"]' -c nimbus.seeds='["storm-nimbus"]'
    deploy: 
      resources:
        limits:
          cpus: '0.5'
          memory: 2G

  storm-ui:
    image: library/storm:1.2.3
    restart: always
    networks:
      - crawler
    ports:
      - 8100:8080
    depends_on:
      - "storm-nimbus"
    command: storm ui -c nimbus.seeds='["storm-nimbus"]'

  crawler-selenium: 
    image: selenium/standalone-chrome:3.141.59-20201010
    restart: unless-stopped
    privileged: true
    networks:
      - crawler
    expose:
      - 4444
    volumes:
      - "crawler-selenium-data:/dev/shm:rw"
    environment:
      SE_OPTS: "-sessionTimeout 315360000"

  document-converter:
    image: kybeidosci/vdpp-document-converter:demonstrator_v0.0.1
    restart: unless-stopped
    privileged: true
    networks:
      - backend
    depends_on:
      - kafka
      - schema-registry
    environment:
      DYNACONF_KAFKA_BROKER_URLS: kafka:29092
      DYNACONF_KAFKA_SCHEMA_REGISTRY_URL: http://schema-registry:8081
    command: [ "./wait-for-it.sh", "schema-registry:8081", "-s", "-t", "120", "--", "./install_lib_and_start.sh" ]

  document-transfer-service:
    image: kybeidosci/vdpp-document-transfer:demonstrator_v0.0.1
    restart: unless-stopped
    privileged: true
    networks:
      - backend
    depends_on:
      - kafka
      - schema-registry
    environment:
      DYNACONF_KAFKA_BROKER_URLS: kafka:29092
      DYNACONF_KAFKA_SCHEMA_REGISTRY_URL: http://schema-registry:8081
    command: ["./wait-for-it.sh", "schema-registry:8081", "-s", "-t", "120", "--", "./install_lib_and_start.sh"]



networks:
  backend:
  crawler:

volumes:
  mongo-data:
  kafka-zookeeper-data:
  kafka-zookeeper-logs:
  storm-zookeeper-data:
  storm-zookeeper-logs:
  kafka-data:
  storm-supervisor-data:
  storm-nimbus-data:
  crawler-selenium-data:
  seaweed-master-data:
  seaweed-volume-data:
